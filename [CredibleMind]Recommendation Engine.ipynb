{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xyqOzsOgonC1"},"source":["The notebook is divided into three parts:\n","\n","**Section 1:** Data set-up. This part is to set up the clean-all data that is going to be used to train the model with. This separate step enables the change in data source.\n","\n","**Section 2:** Training model (with test report). This part is to generate a machine learning model that is to be used in the last section to predict the category. This step enables the change in model used (Logistic Regression or Naive Bayes..) and change in the category that needs to be predicted.\n","\n","**Section 3:** A small helper function to predict the category. This steps enables testing on multiple data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Dhf6cyr54sM","executionInfo":{"status":"ok","timestamp":1629397610811,"user_tz":420,"elapsed":3279,"user":{"displayName":"Zain Ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrTStEmkaGMU8tFZfPOsnV0sD1z59eviGTj1jM=s64","userId":"03890621630760058916"}},"outputId":"045b1386-c276-490f-ea93-9db2ca5983ca"},"source":["!pip install youtube_transcript_api\n","import psycopg2\n","import pandas as pd\n","import pandas_gbq\n","import numpy as np\n","import json\n","import nltk\n","import re\n","import pickle\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer as tf\n","from sklearn import feature_extraction, model_selection, feature_selection, pipeline, metrics\n","#from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: youtube_transcript_api in /usr/local/lib/python3.7/dist-packages (0.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from youtube_transcript_api) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->youtube_transcript_api) (1.24.3)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jExhykinmwwD"},"source":["conn_string = \"host=\"+ 'lifeview-dev.c7b1kmie3jdi.us-west-2.rds.amazonaws.com' + \" port=\" + \"5432\" + \" dbname=\"+ 'lifeview_content' + \" user=\" + 'interns' + \" password=\" + 'FoolishSamba66^^'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5L1uMTxqmz_p"},"source":["# testing out audience\n","classifier_type = \"Audience\"\n","entry_id = \"44SXOeFWwh0L8uX32lrNwA\"\n","categories = ['Teens','College Students','Parents']\n","\n","#testing out topics\n","#classifier_type = \"Topics\"\n","#categories = ['Mental Health', 'Personal Development', 'Relationships']\n","#entry_id = '2LQJ7KQATrqaiJGKucgXCZ'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wZizdh3LDeEw"},"source":["#**Section 1 : Helper Functions for Cleaning Dataframe**"]},{"cell_type":"markdown","metadata":{"id":"5lBGZyRU-WdM"},"source":["Convert JSON format to interpretable data"]},{"cell_type":"code","metadata":{"id":"AcIR3JtT-VhV"},"source":["#\"data_convert\" method\n","def data_convert(df):\n","  if (classifier_type == 'Audience'):\n","    data_list = list(df.loc[:, \"audiencetype\"].values)\n","  if (classifier_type == 'Topics'):\n","    data_list = list(df.loc[:, \"topics\"].values)\n","\n","  entries = []\n","  for a in data_list:\n","    if a == None:\n","      entries.append('None')\n","    if a != None:\n","      entries.append(json.loads(a))\n","  new_data = []\n","  for a in entries:\n","    if a == 'None':\n","      new_data.append('None')\n","    else:\n","      if len(a) > 1:\n","        new_data.append(\", \".join([a[b].get('sys').get('id') for b in range(len(a))]))\n","      else:\n","        new_data.append(a[0].get('sys').get('id'))\n","\n","  if (classifier_type == 'Audience'):\n","    df['audiencetype'] = new_data\n","    df_split = df.assign(audiencetype=df['audiencetype'].str.split(', ')).explode('audiencetype')\n","  if (classifier_type == 'Topics'):\n","    df['topics'] = new_data\n","    df_split = df.assign(topics=df['topics'].str.split(', ')).explode('topics')\n","\n","  return df_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJfRxFPw-h27"},"source":["#\"desc_convert\" helper function\n","def getVal(data):\n","    this = ''\n","    if(type(data) is dict and data.get('value') == None):\n","     return getVal(data.get('content'))\n","    if(type(data) is list):\n","       for a in data:\n","        if type(a) is dict and a.get('value') != None:\n","          this += a.get('value')\n","        else:\n","         while type(a) is dict and a.get('value') == None:\n","           a = a.get('content')\n","         if a[0].get('value') == None:\n","           return this\n","         this += a[0].get('value')\n","    return this"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QfOvCNgb-skx"},"source":["#\"desc_convert\" function\n","def desc_convert(df):\n","  #desc = pd.read_sql_query(\"SELECT description FROM video_flat\", con=conn)\n","  descrip = list(df.loc[:, \"description\"].values)\n","  desc_entries = []\n","  for a in descrip:\n","    if a == None:\n","      desc_entries.append('None')\n","    if a != None:\n","      desc_entries.append(json.loads(a))\n","  new_desc = []\n","  for a in desc_entries:\n","    if a == 'None':\n","      new_desc.append('None')\n","    if(a != 'None'):\n","      b = a.get('content')\n","      new_desc.append(\" \".join([getVal(c) for c in b]))\n","  df['description'] = new_desc\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vdS9jjSIDnA0"},"source":["# Clean the table to have data necessary for training classification models\n","# Then merge the topic ids to match with their actual name\n","def clean_merge(df, classes):\n","  df_rev = df.loc[df['reviewstatus'] == 'Accepted']\n","  df_re = df_rev.drop(columns = ['reviewstatus'])\n","  if (classifier_type == 'Audience'):\n","    df_none = df_re.loc[df_re['audiencetype'] != 'None']\n","    merged = pd.merge(classes, df_none, on='audiencetype', how='right')\n","    clean = merged.drop(columns = 'audiencetype')\n","    df_merge = clean.rename(columns = {'name': 'audiencetype', 'title_y': 'title'})\n","    df_clean = df_merge.rename(columns = {'name_x': 'audiencetype', 'name_y': 'title'}) #needed to rename df_apps dataframe\n","\n","  if (classifier_type == 'Topics'):\n","    df_none = df_re.loc[df_re['topics'] != 'None']\n","    merged = pd.merge(classes, df_none, on='topics', how='right')\n","    clean = merged.drop(columns = 'topics')\n","    df_clean = clean.rename(columns = {'title_x': 'topics', 'title_y': 'title'})\n","\n","  return df_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LA_SneJwENxm"},"source":["def preprocess_text(text, lst_stopwords=None):\n","    # clean (convert to lowercase and remove punctuations and characters and then strip)\n","    text1 = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n","    text2 = re.sub('[0-9]+', '', text1)\n","    # Tokenize (convert from string to list)\n","    lst_text = text2.split()    ## remove Stopwords\n","    if lst_stopwords is not None:\n","        lst_text = [word for word in lst_text if word not in\n","                    lst_stopwords]\n","\n","    # back to string from list\n","    text = \" \".join(lst_text)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uE7FV2nyExiw"},"source":["#**Section 2 : Function for creating model with feature selection (and testing)**"]},{"cell_type":"code","metadata":{"id":"APpOPRUtE40m"},"source":["def feature_selection_cm(df, classifier, print_detail):\n","  X = df.text.values\n","  if (classifier_type == 'Audience'):\n","     y = df.audiencetype.values\n","  if (classifier_type == 'Topics'):\n","     y = df.topics.values\n","  df_train, df_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n","\n","  vectorizer = feature_extraction.text.TfidfVectorizer()\n","  corpus = df_train\n","  vectorizer.fit(corpus)\n","  X_train = vectorizer.transform(corpus)\n","\n","  y = y_train\n","  X_names = vectorizer.get_feature_names()\n","  p_value_limit = 0.85\n","  dtf_features = pd.DataFrame()\n","\n","  for cat in np.unique(y):\n","      chi2, p = feature_selection.chi2(X_train, y==cat)\n","      dtf_features = dtf_features.append(pd.DataFrame(\n","                    {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n","      dtf_features = dtf_features.sort_values([\"y\",\"score\"],\n","                      ascending=[True,False])\n","      dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n","\n","  X_names = dtf_features[\"feature\"].unique().tolist()\n","\n","  vectorizer = tf(vocabulary=X_names)\n","  vectorizer.fit(corpus)\n","  X_train = vectorizer.transform(corpus)\n","\n","  model = pipeline.Pipeline([(\"vectorizer\", vectorizer),\n","                            (\"classifier\", classifier)])## train classifier\n","  model[\"classifier\"].fit(X_train, y_train)## test\n","  predicted = model.predict(df_test)\n","  predicted_prob = model.predict_proba(df_test)\n","\n","  classes = np.unique(y_test)\n","  y_test_array = pd.get_dummies(y_test, drop_first=False).values\n","    ## Accuracy, Precision, Recall\n","  accuracy = metrics.accuracy_score(y_test, predicted)\n","\n","  if (print_detail == True):\n","    print_details(accuracy, y_test, predicted)\n","\n","  import pickle\n","  filename = 'finalized_model.sav'\n","  pickle.dump(model, open(filename, 'wb'))\n","\n","  return vectorizer, model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0miGppgyGp2"},"source":["def print_details(accuracy, y_test, predicted):\n","  print(\"Accuracy:\",  round(accuracy,2))\n","  print(\"Detail:\")\n","  print(metrics.classification_report(y_test, predicted))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJ_gkF8rpd7_"},"source":["# **Section 3 : Function for loading saved model to predict output (for single entry_id)**"]},{"cell_type":"code","metadata":{"id":"zXVESeUxLZnF"},"source":["def predict_output(df, id):\n","  #vectorizer, model = feature_selection_cm(df, classifier = LogisticRegression(warm_start=True), print_detail = print_detail)\n","  loaded_model = pickle.load(open('finalized_model.sav', 'rb'))\n","  new_df = df[df['entry_id'] == id]\n","  X = new_df.text.tolist()\n","  vectorizer.fit_transform(X)\n","  output = loaded_model.predict(X)\n","  return output[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cA-OuR2p-v7X"},"source":["#**Main function**"]},{"cell_type":"code","metadata":{"id":"PsKREW0z8Li-"},"source":["def at_topics_model(conn, classifier_type, entry_id, categories):\n","  #Connect to Postgre to get data (all are flat tables)\n","  conn = psycopg2.connect(conn)\n","  if (classifier_type == 'Audience'):\n","    df_videos = pd.read_sql_query('SELECT entry_id, title, reviewstatus, audiencetype, description, shortdescription, youtubeid FROM video_flat', con=conn)\n","    df_podcasts = pd.read_sql_query('SELECT entry_id, title, reviewstatus, audiencetype, description, shortdescription, listennotesdescription FROM podcast_flat', con=conn)\n","    df_books = pd.read_sql_query('SELECT entry_id, title, reviewstatus, audiencetype, description, goodreadsdescription FROM book_flat', con=conn)\n","    df_apps = pd.read_sql_query('SELECT entry_id, name, reviewstatus, audiencetype, description, shortdescription, highlights FROM application_flat', con=conn)\n","    audience = pd.read_sql_query(\"SELECT entry_id, ((audience.raw -> 'fields'::text) -> 'name'::text) ->> 'en-US'::text AS name FROM audience\", con=conn)\n","    audience = audience.rename(columns={'entry_id':'audiencetype'})\n","  if (classifier_type == 'Topics'):\n","    df_videos = pd.read_sql_query('SELECT entry_id, title, reviewstatus, topics, description, shortdescription, youtubeid FROM video_flat', con=conn)\n","    df_podcasts = pd.read_sql_query('SELECT entry_id, title, reviewstatus, topics, description, shortdescription, listennotesdescription FROM podcast_flat', con=conn)\n","    df_books = pd.read_sql_query('SELECT entry_id, title, reviewstatus, topics, description, goodreadsdescription FROM book_flat', con=conn)\n","    df_apps = pd.read_sql_query('SELECT entry_id, name, reviewstatus, topics, description, shortdescription, highlights FROM application_flat', con=conn)\n","    df_apps = df_apps.rename(columns = {'name': 'title'})\n","    topics = pd.read_sql_query(\"SELECT entry_id, title FROM topics_flat\", con=conn)\n","    topics = topics.rename(columns={'entry_id':'topics'})\n","\n","  #Convert JSON Format to interretable data\n","  df_videos = data_convert(df_videos)\n","  df_books = data_convert(df_books)\n","  df_podcasts = data_convert(df_podcasts)\n","  df_apps = data_convert(df_apps)\n","\n","  df_videos = desc_convert(df_videos)\n","  df_books = desc_convert(df_books)\n","  df_podcasts = desc_convert(df_podcasts)\n","  df_apps = desc_convert(df_apps)\n","\n","  #Clean pandas dataframe\n","  if (classifier_type == 'Audience'):\n","    df_videos = clean_merge(df_videos, audience)\n","    df_books = clean_merge(df_books, audience)\n","    df_podcasts = clean_merge(df_podcasts, audience)\n","    df_apps = clean_merge(df_apps, audience)\n","  if (classifier_type == 'Topics'):\n","    df_videos = clean_merge(df_videos, topics)\n","    df_books = clean_merge(df_books, topics)\n","    df_podcasts = clean_merge(df_podcasts, topics)\n","    df_apps = clean_merge(df_apps, topics)\n","\n","  #Preprocess Text\n","  df_videos = df_videos.fillna(value='None')\n","  df_podcasts = df_podcasts.fillna(value='None')\n","  df_books = df_books.fillna(value='None')\n","  df_apps = df_apps.fillna(value='None')\n","\n","  text_videos = df_videos.title + ' ' + df_videos.description + ' ' + df_videos.shortdescription #+ ' ' + df_videos.transcript\n","  text_podcasts = df_podcasts.title + ' ' + df_podcasts.description + ' ' + df_podcasts.shortdescription + ' ' + df_podcasts.listennotesdescription\n","  text_books = df_books.title + ' ' + df_books.description + ' ' + df_books.goodreadsdescription\n","  text_apps = df_apps.title + ' ' + df_apps.description + ' ' + df_apps.shortdescription + df_apps.highlights\n","  df_videos['text'] = text_videos.to_list()\n","  df_podcasts['text'] = text_podcasts.to_list()\n","  df_books['text'] = text_books.to_list()\n","  df_apps['text'] = text_apps.to_list()\n","\n","  lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n","  df_videos[\"text\"] = df_videos[\"text\"].apply(lambda x: preprocess_text(x, lst_stopwords=lst_stopwords))\n","  df_podcasts[\"text\"] = df_podcasts[\"text\"].apply(lambda x: preprocess_text(x, lst_stopwords=lst_stopwords))\n","  df_books[\"text\"] = df_books[\"text\"].apply(lambda x: preprocess_text(x, lst_stopwords=lst_stopwords))\n","  df_apps[\"text\"] = df_apps[\"text\"].apply(lambda x: preprocess_text(x, lst_stopwords=lst_stopwords))\n","\n","  frames = [df_videos, df_podcasts, df_books, df_apps]\n","  df_clean_all = pd.concat(frames)\n","\n","  # transform data into only the categories we want for training\n","  if classifier_type == 'Audience':\n","    df = df_clean_all.loc[df_clean_all['audiencetype'].isin(categories)]\n","  if classifier_type == 'Topics':\n","    df = df_clean_all.loc[df_clean_all['topics'].isin(categories)]\n","\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WyYbxNHGqMfF"},"source":["# **Testing Functions/Implementation**"]},{"cell_type":"code","metadata":{"id":"hqKKPTl9yn_q"},"source":["# takes in string value for connection to PostgreSQL data,\n","# the string value for which classification model you want to train ('Audience' or 'Topics'),\n","# string entry_id of a data entry that you want predicted value of ('2LQJ7KQATrqaiJGKucgXCZ'),\n","# and array of categories/classes you want to train (['Depression', 'Anxiety'])\n","# returns clean dataframe\n","\n","df = at_topics_model(conn_string, classifier_type, entry_id, categories)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VMUEZca9yxRl","executionInfo":{"status":"ok","timestamp":1629397719330,"user_tz":420,"elapsed":382,"user":{"displayName":"Zain Ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrTStEmkaGMU8tFZfPOsnV0sD1z59eviGTj1jM=s64","userId":"03890621630760058916"}},"outputId":"b70d9b1e-169c-4d45-bac4-bcb4bd1e8c62"},"source":["# creates and saves the model\n","vectorizer, model = feature_selection_cm(df, classifier = LogisticRegression(warm_start=True), print_detail = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.75\n","Detail:\n","                  precision    recall  f1-score   support\n","\n","College Students       0.29      0.06      0.10        33\n","         Parents       0.87      0.88      0.87       114\n","           Teens       0.65      0.84      0.74        88\n","\n","        accuracy                           0.75       235\n","       macro avg       0.60      0.59      0.57       235\n","    weighted avg       0.71      0.75      0.71       235\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"IQ07Lhuoyyon","executionInfo":{"status":"ok","timestamp":1629397723833,"user_tz":420,"elapsed":151,"user":{"displayName":"Zain Ali","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrTStEmkaGMU8tFZfPOsnV0sD1z59eviGTj1jM=s64","userId":"03890621630760058916"}},"outputId":"e67f1a20-17ee-43da-ad0a-9d884c23c7af"},"source":["# loads saved model, predicts desired output for entry_id given\n","output = predict_output(df, entry_id)\n","output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'College Students'"]},"metadata":{},"execution_count":28}]}]}